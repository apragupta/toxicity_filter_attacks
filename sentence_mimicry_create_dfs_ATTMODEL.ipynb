{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4a350a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T07:09:07.158000Z",
     "start_time": "2021-12-06T07:09:07.153999Z"
    }
   },
   "source": [
    "# This attack simply inserts high-confidence 'positive' words based on queries from the test set to the model on sentences around that are negative. \n",
    "\n",
    "- This is a black box attack, assumes only access to confidence predictions and some preexising dataset of sentences, which may be benign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b62a6",
   "metadata": {},
   "source": [
    "## STEPS\n",
    "1. Load all test sentences\n",
    "2. Score them individually\n",
    "3. Get top N (10?) benign, with high-confidence \n",
    "4. Generate attack tests (vary combinations of (`a=0,1,2` and `b=0,1,2`):\n",
    " - Adding `a` sentences before and `b` sentences after, chosen randomly from step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a66eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T09:20:07.274883Z",
     "start_time": "2021-12-06T09:20:07.258884Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from experiments import params\n",
    "from data_utils_v2.data_helpers import genFeatures, loadVocabEmb,genPOSFeatures\n",
    "from model.abuse_classifier import AbuseClassifier\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8006169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model type and attention loss type we are testing\n",
    "\n",
    "# MODEL_TYPE = \"model_att=encoded_checkpoints\"\n",
    "MODEL_TYPE = \"model_att=encoded_checkpoints\"\n",
    "ATTENTION_LOSS_TYPE = \"encoded\"\n",
    "comm_or_sent = \"sent\"\n",
    "\n",
    "#if model type is no attention, attention lambda should be 0\n",
    "if MODEL_TYPE == \"model_noatt_checkpoints\":\n",
    "    attention_lambda = 0.0\n",
    "    ATTENTION_LOSS_TYPE = \"none\"\n",
    "else:\n",
    "    attention_lambda = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69cc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these thresholds were found by seeing which threshold yielded the best f1 score in the training data\n",
    "#best threshold for attention model and non attention model came out to be the same\n",
    "BEST_THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82e0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"pos_vocab_size\", 26, \"Vocab size of POS tags\")\n",
    "tf.flags.DEFINE_integer(\"pos_embedding_dim\", 25, \"Dimensionality of pos tag embedding (default: 20)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 1.0, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"attention_lambda\", attention_lambda, \"Supervised attention lambda (default: 0.05)\")\n",
    "tf.flags.DEFINE_string(\"attention_loss_type\", ATTENTION_LOSS_TYPE, \"loss function of attention\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.02, \"L2 regularization lambda (default: 0.05)\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\", 300, \"Dimensionality of RNN cell (default: 300)\")\n",
    "tf.flags.DEFINE_integer(\"pos_hidden_size\", 25, \"Dimensionality of POS-RNN cell\")\n",
    "tf.flags.DEFINE_integer(\"attention_size\", 20, \"Dimensionality of attention scheme (default: 50)\")\n",
    "tf.flags.DEFINE_boolean(\"use_pos_flag\", True, \"use the sequence of POS tags\")\n",
    "# Training parameters -- evaluate_every should be 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 32)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 60, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 500000, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_float(\"train_ratio\", 1.0, \"Ratio of training data\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint\", '', \"model\")\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a1bf2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T12:03:09.413747Z",
     "start_time": "2021-12-06T12:03:09.404748Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model from model_att=encoded_checkpoints\n"
     ]
    }
   ],
   "source": [
    "model_save_folder_name = MODEL_TYPE\n",
    "print(f\"Running model from {model_save_folder_name}\")\n",
    "model_folder_path = 'model_new'\n",
    "checkpoint_dir = os.path.abspath(os.path.join(model_folder_path, model_save_folder_name))\n",
    "model_path = os.path.join(checkpoint_dir, \"best_model\")\n",
    "\n",
    "assert os.path.isdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b7f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preprocessing/dump/ and dump from preprocessing/dump/\n"
     ]
    }
   ],
   "source": [
    "data_path = 'preprocessing/dump/'\n",
    "dump_path = \"preprocessing/dump/\"\n",
    "print(f\"Using data from {data_path} and dump from {dump_path}\")\n",
    "assert os.path.isdir(data_path)\n",
    "assert os.path.isdir(dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d095ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(dump_folder):\n",
    "    vocabulary, pos_vocabulary, init_embed = loadVocabEmb(dump_folder)\n",
    "    return vocabulary, pos_vocabulary, init_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d6f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "max_sent_len = 100\n",
    "unk = \"<UNK>\"\n",
    "pad = \"<PAD/>\"\n",
    "emb_dim = 300\n",
    "\n",
    "def load_data_sents_only_side(dump_folder, data_folder, data_type, only_negative: bool, verbose=True, type=\"sentence\"):\n",
    "    assert data_type in [\"train\", \"test\"]\n",
    "    with open(os.path.join(dump_folder, \"vocab.pkl\"), \"rb\") as handle:\n",
    "        vocabulary = pickle.load(handle)\n",
    "    with open(os.path.join(dump_folder, \"pos_vocab.pkl\"), \"rb\") as handle:\n",
    "        pos_vocabulary = pickle.load(handle)\n",
    "        \n",
    "    df_path = os.path.join(data_folder, data_type + f\"_{type}_df\")\n",
    "    data_df = pd.read_pickle(df_path)\n",
    "    data_df = data_df[data_df[\"Abusive\"] == (\"Yes\" if only_negative else \"No\")]\n",
    "    \n",
    "    sentences = data_df[\"tokenized\"].to_list()\n",
    "    if(type==\"comments\"):\n",
    "        labels = data_df[\"merged_label\"].to_list()\n",
    "    else:\n",
    "        labels = data_df[\"binarized_label\"].to_list()\n",
    "\n",
    "    pos_sentences = data_df[\"pos_tags\"].to_list()\n",
    "    attention = data_df[\"attention\"].to_list()\n",
    "\n",
    "    # generate features & labels\n",
    "    x, length, attention = genFeatures(sentences, attention, max_sent_len, vocabulary)\n",
    "    pos, pos_length = genPOSFeatures(pos_sentences, max_sent_len, pos_vocabulary)\n",
    "    y = np.array(labels)\n",
    "    if verbose:\n",
    "        print(\"load {} data, input sent size: {}, input POS size: {}, label size: {}\".format(\n",
    "            data_type, np.array(x).shape, np.array(pos).shape, np.array(y).shape))\n",
    "    return x, length, attention, pos, pos_length, y, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "287e28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_sent(model_path, dump_folder_path, data_folder_path, only_neg_sent, data_type=\"test\"):\n",
    "    with open(os.path.join(dump_folder_path, \"norm_init_embed.pkl\"), \"rb\") as handle:\n",
    "        init_embed = pickle.load(handle)\n",
    "\n",
    "    \n",
    "    \n",
    "    x_test, length_test, attention_test, pos_test, pos_length_test, y_test, sentences = load_data_sents_only_side(dump_folder_path, data_folder_path, data_type, only_neg_sent, verbose=False)\n",
    "    \n",
    "    len_data = len(x_test)\n",
    "    print(f\"Running model on {len_data} {data_type} samples\")\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            model = AbuseClassifier(\n",
    "                max_sequence_length=params.max_sent_len,\n",
    "                num_classes=2,\n",
    "                pos_vocab_size=FLAGS.pos_vocab_size,\n",
    "                init_embed=init_embed,\n",
    "                hidden_size=FLAGS.hidden_size,\n",
    "                attention_size=FLAGS.attention_size,\n",
    "                keep_prob=FLAGS.dropout_keep_prob,\n",
    "                attention_lambda=FLAGS.attention_lambda,\n",
    "                attention_loss_type=FLAGS.attention_loss_type,\n",
    "                l2_reg_lambda=0.1,\n",
    "                use_pos_flag=FLAGS.use_pos_flag)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            saver.restore(sess, model_path)\n",
    "\n",
    "            dev_scores = []\n",
    "            dev_confidences = []\n",
    "            alphas = []\n",
    "            pos = 0\n",
    "            gap = 50\n",
    "            while pos < len(x_test):\n",
    "                x_batch = x_test[pos:pos + gap]\n",
    "                pos_batch = pos_test[pos:pos + gap]\n",
    "                y_batch = y_test[pos:pos + gap]\n",
    "                length_batch = length_test[pos:pos + gap]\n",
    "                pos_length_batch = pos_length_test[pos:pos + gap]\n",
    "                pos += gap\n",
    "                # score sentences\n",
    "                feed_dict = {\n",
    "                    model.input_word: x_batch,\n",
    "                    model.input_pos: pos_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.sequence_length: length_batch,\n",
    "                    model.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, scores, alpha = sess.run([global_step, model.prob, model.alphas], feed_dict)\n",
    "                dev_confidences = dev_confidences + list([[s[0],s[1]] for s in scores])\n",
    "                dev_scores = dev_scores + list([s[0] for s in scores])\n",
    "                alphas = alphas + list(alpha)\n",
    "                \n",
    "    return (dev_confidences, dev_scores, y_test, alphas), (x_test, length_test, attention_test, pos_test, pos_length_test, y_test, sentences)\n",
    "\n",
    "def get_predictions_sent_only_pos(model_path, dump_folder_path, data_folder_path, data_type=\"test\"):\n",
    "    return get_predictions_sent(model_path, dump_folder_path, data_folder_path, False, data_type=\"test\")\n",
    "        \n",
    "def get_predictions_sent_only_neg(model_path, dump_folder_path, data_folder_path, data_type=\"test\"):\n",
    "    return get_predictions_sent(model_path, dump_folder_path, data_folder_path, True, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81c61792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded sent: (4461, 100)\n",
      "feature shape: (4461, 100)\n",
      "padded pos sentences: (4461, 100)\n",
      "debug padded_pos_sentences: ['V', 'A', 'N', 'O', 'V', 'E', 'D', 'N', 'V', 'O']\n",
      "pos feature shape: (4461, 100)\n",
      "Running model on 4461 test samples\n",
      "WARNING:tensorflow:From /Users/pablokvitca/Projects/Academic/NEU/CY7790/toxicity_filter_attacks/model/abuse_classifier.py:48: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/pablokvitca/Projects/Academic/NEU/CY7790/toxicity_filter_attacks/model/abuse_classifier.py:50: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/pablokvitca/anaconda3/envs/toxicity_filter_attacks_36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/pablokvitca/anaconda3/envs/toxicity_filter_attacks_36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/pablokvitca/anaconda3/envs/toxicity_filter_attacks_36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/pablokvitca/anaconda3/envs/toxicity_filter_attacks_36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/pablokvitca/Projects/Academic/NEU/CY7790/toxicity_filter_attacks/model/abuse_classifier.py:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/pablokvitca/Projects/Academic/NEU/CY7790/toxicity_filter_attacks/model/abuse_classifier.py:75: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Supervised attention with encoded loss.\n",
      "WARNING:tensorflow:From <ipython-input-10-6f61cc6ccc36>:33: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From /Users/pablokvitca/anaconda3/envs/toxicity_filter_attacks_36/lib/python3.6/site-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "INFO:tensorflow:Restoring parameters from /Users/pablokvitca/Projects/Academic/NEU/CY7790/toxicity_filter_attacks/model_new/model_att=encoded_checkpoints/best_model\n"
     ]
    }
   ],
   "source": [
    "predictions_only_pos, data_only_pos = get_predictions_sent_only_pos(model_path, dump_path, data_path, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e03f9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_confidences_only_pos, dev_scores_only_pos, y_test_only_pos, alphas_only_pos = predictions_only_pos\n",
    "dev_confidences_only_pos = np.array(dev_confidences_only_pos)\n",
    "dev_scores_only_pos = np.array(dev_scores_only_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd56262",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_only_pos, length_test_only_pos, attention_test_only_pos, \\\n",
    "pos_test_only_pos, pos_length_test_only_pos, y_test_only_pos, sentences_only_pos = data_only_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0654d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6519251 , 0.90579915, 0.90477836, ..., 0.90324706, 0.90417606,\n",
       "       0.9002203 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_conf_benign = dev_confidences_only_pos[:,1]\n",
    "dev_conf_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c864de",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_lists = zip(*[\n",
    "    list(l) for l in [\n",
    "        dev_conf_benign, \n",
    "        x_test_only_pos, \n",
    "        length_test_only_pos, \n",
    "        attention_test_only_pos, \n",
    "        pos_test_only_pos, \n",
    "        pos_length_test_only_pos, \n",
    "        y_test_only_pos, \n",
    "        sentences_only_pos\n",
    "    ]\n",
    "])\n",
    "sorted_lists = tuple(zip(*sorted(zipped_lists, key=lambda x: x[0])))\n",
    "\n",
    "dev_conf_benign_sorted, \\\n",
    "x_test_only_pos_sorted, \\\n",
    "length_test_only_pos_sorted, \\\n",
    "attention_test_only_pos_sorted, \\\n",
    "pos_test_only_pos_sorted, \\\n",
    "pos_length_test_only_pos_sorted, \\\n",
    "y_test_only_pos_sorted, \\\n",
    "sentences_only_pos_sorted = sorted_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cef445",
   "metadata": {},
   "source": [
    "# Attack\n",
    "\n",
    "### Make new Comment DF with modified comments\n",
    "\n",
    "#### STEPS:\n",
    "1. Load the original comments DF\n",
    "2. Get top N positive sentences \n",
    "3. Loop for each for each comment ID\n",
    "4. Add a sentences before orignal comment\n",
    "5. Add b sentences after original comment\n",
    "6. Save new DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6af2ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03762428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adv_examples_df(data_path, select_top_n, a_sentences_before, b_sentences_after, sorted_lists, save_path):\n",
    "    # STEP 1: Load the original comments DF\n",
    "    df_path = os.path.join(data_path, \"test_comments_df\")\n",
    "    data_df = pd.read_pickle(df_path)\n",
    "    data_df_only_neg = data_df[data_df[\"merged_label\"].apply(lambda lbl: lbl[0] == 1)]\n",
    "    \n",
    "    # STEP 2: Get top N positive sentences \n",
    "    attack_data_dev_conf_benign = sorted_lists[0][-select_top_n:]\n",
    "    attack_data_x_test_benign = sorted_lists[1][-select_top_n:]\n",
    "    attack_data_length_test_benign = sorted_lists[2][-select_top_n:]\n",
    "    attack_data_attention_test_benign = sorted_lists[3][-select_top_n:]\n",
    "    attack_data_pos_test_benign = sorted_lists[4][-select_top_n:]\n",
    "    attack_data_pos_length_test_benign = sorted_lists[5][-select_top_n:]\n",
    "    attack_data_y_test_benign = sorted_lists[6][-select_top_n:]\n",
    "    attack_data_sentences_tokenized_benign = sorted_lists[7][-select_top_n:]\n",
    "    attack_data_sentences_benign = [\" \".join(sent) for sent in attack_data_sentences_tokenized_benign]\n",
    "    \n",
    "    # STEP 3: Loop for each for each comment ID\n",
    "    neg_comment_count = data_df_only_neg.shape[0]\n",
    "    for comment_index in range(neg_comment_count):\n",
    "        row = data_df_only_neg.iloc[comment_index]\n",
    "\n",
    "        # STEP 4, 5: Add a sentences before orignal comment, Add b sentences after orignal comment\n",
    "\n",
    "        sentences_before = [np.random.choice(range(select_top_n)) for _ in range(a_sentences_before)]\n",
    "        sentences_after = [np.random.choice(range(select_top_n)) for _ in range(b_sentences_after)]\n",
    "\n",
    "        comment_before = [attack_data_sentences_benign[i] for i in sentences_before]\n",
    "        comment_after = [attack_data_sentences_benign[i] for i in sentences_after]\n",
    "        row[\"Comment\"] = comment_before + row[\"Comment\"] + comment_after\n",
    "\n",
    "        row[\"labels\"] = ([0] * a_sentences_before) + row[\"labels\"] + ([0] * b_sentences_after)\n",
    "        row[\"merged_comment\"] = \". \".join(row[\"Comment\"])\n",
    "\n",
    "        # Labelled as toxic, this would be the \"human label\", we want the model to get this wrong\n",
    "        row[\"merged_label\"] = [1, 0] \n",
    "\n",
    "        tokenized_before = flatten([attack_data_sentences_tokenized_benign[i] for i in sentences_before])\n",
    "        tokenized_after = flatten([attack_data_sentences_tokenized_benign[i] for i in sentences_after])\n",
    "\n",
    "        row[\"tokenized\"] = tokenized_before + row[\"tokenized\"] + tokenized_after\n",
    "\n",
    "        row[\"attention\"] = [] # ignore this\n",
    "\n",
    "        pos_before = flatten([attack_data_pos_test_benign[i] for i in sentences_before])\n",
    "        pos_after = flatten([attack_data_pos_test_benign[i] for i in sentences_after])\n",
    "        row[\"pos_tags\"] = pos_before + row[\"pos_tags\"] + pos_after\n",
    "        \n",
    "    # STEP 6: Save new DF\n",
    "    save_file_name = f\"adv_sent_mimicry_n{select_top_n}_a{a_sentences_before}_b{b_sentences_after}_ENCATT_df\"\n",
    "    data_df_only_neg.to_pickle(f\"{save_path}/{save_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ca7e1ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_n_options = [10, 50, 100]\n",
    "ab_sent_options = [0, 1, 2, 3, 5]\n",
    "for n in top_n_options:\n",
    "    for a in ab_sent_options:\n",
    "        for b in ab_sent_options:\n",
    "            make_adv_examples_df(\n",
    "                'preprocessing/dump/',  # DATA PATH\n",
    "                n, a, b,                # select_top_n, a_sentences_before, b_sentences_after                \n",
    "                sorted_lists,           # SORTED DATA\n",
    "                'adv_sentence_mimicry'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398df198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
