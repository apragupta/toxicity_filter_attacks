{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4e2cff",
   "metadata": {},
   "source": [
    "# Following instructions from Gong et al to train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3271b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.4 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35db9d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T07:01:25.963364Z",
     "start_time": "2021-11-19T07:01:23.121456Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import param as param\n",
    "from data_utils import data_helpers, tag_data_helpers\n",
    "from model.abuse_classifier import AbuseClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5892efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T07:01:28.075791Z",
     "start_time": "2021-11-19T07:01:26.601789Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded sent: (9232, 100)\n",
      "feature shape: (9232, 100)\n",
      "padded pos sentences: (9232, 100)\n",
      "debug padded_pos_sentences: ['&', 'O', 'V', 'D', 'N', 'N', ',', '&', 'O', 'V']\n",
      "pos feature shape: (9232, 100)\n",
      "load train data, input sent size: (9232, 100), input POS size: (9232, 100), label size: (9232, 2)\n",
      "split into train (7385 examples) and dev sets (1847 examples)\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary and initial embeddings\n",
    "vocabulary, pos_vocabulary, init_embed = data_helpers.loadVocabEmb()\n",
    "\n",
    "pos_vocab_size = len(pos_vocabulary)\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "x_train, length_train, attention_train, pos_train, pos_length_train, y_train, \\\n",
    "x_dev, length_dev, attention_dev, pos_dev, pos_length_dev, y_dev \\\n",
    "    = data_helpers.loadTrainData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5827bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T20:45:51.508726Z",
     "start_time": "2021-11-19T20:45:48.237737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Tensorflow version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train abusive language classifier\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import os\n",
    "\n",
    "import param as param\n",
    "from data_utils import data_helpers\n",
    "from model.abuse_classifier import AbuseClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.app import flags\n",
    "\n",
    "tf.random.set_seed(111)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Tensorflow version: \",tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1c5689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T20:46:36.766987Z",
     "start_time": "2021-11-19T20:46:36.740986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 128)\")\n",
    "flags.DEFINE_integer(\"pos_vocab_size\", 26, \"Vocab size of POS tags\")\n",
    "flags.DEFINE_integer(\"pos_embedding_dim\", 25, \"Dimensionality of pos tag embedding (default: 20)\")\n",
    "flags.DEFINE_float(\"dropout_keep_prob\", 0.99, \"Dropout keep probability (default: 0.5)\")\n",
    "flags.DEFINE_float(\"attention_lambda\", 0.2, \"Supervised attention lambda (default: 0.05)\")\n",
    "flags.DEFINE_string(\"attention_loss_type\", 'encoded', \"loss function of attention\")\n",
    "flags.DEFINE_float(\"l2_reg_lambda\", 0.02, \"L2 regularizaion lambda (default: 0.05)\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 300, \"Dimensionality of RNN cell (default: 300)\")\n",
    "flags.DEFINE_integer(\"pos_hidden_size\", 25, \"Dimensionality of POS-RNN cell\")\n",
    "flags.DEFINE_integer(\"attention_size\", 20, \"Dimensionality of attention scheme (default: 50)\")\n",
    "flags.DEFINE_boolean(\"use_pos_flag\", True, \"use the sequence of POS tags\")\n",
    "# Training parameters -- evaluate_every should be 100\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 32)\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 60, \"Number of training epochs (default: 200)\")\n",
    "flags.DEFINE_integer(\"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "flags.DEFINE_integer(\"checkpoint_every\", 500000, \"Save model after this many steps (default: 100)\")\n",
    "# flags.DEFINE_float(\"train_ratio\", 1.0, \"Ratio of training data\")\n",
    "# Misc Parameters\n",
    "flags.DEFINE_string(\"checkpoint\", '', \"model\")\n",
    "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "#added so it works in command line \n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f00c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T08:17:19.516710Z",
     "start_time": "2021-11-19T08:17:18.867709Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3169c739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T20:46:21.756382Z",
     "start_time": "2021-11-19T20:46:20.337300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded sent: (9232, 100)\n",
      "feature shape: (9232, 100)\n",
      "padded pos sentences: (9232, 100)\n",
      "debug padded_pos_sentences: ['&', 'O', 'V', 'D', 'N', 'N', ',', '&', 'O', 'V']\n",
      "pos feature shape: (9232, 100)\n",
      "load train data, input sent size: (9232, 100), input POS size: (9232, 100), label size: (9232, 2)\n",
      "split into train (7385 examples) and dev sets (1847 examples)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------  load data  -----------------------------\n",
    "vocabulary, pos_vocabulary, init_embed = data_helpers.loadVocabEmb()\n",
    "pos_vocab_size = len(pos_vocabulary)\n",
    "x_train, length_train, attention_train, pos_train, pos_length_train, y_train, \\\n",
    "x_dev, length_dev, attention_dev, pos_dev, pos_length_dev, y_dev \\\n",
    "    = data_helpers.loadTrainData()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92360e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T21:11:52.416188Z",
     "start_time": "2021-11-19T21:09:17.635611Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apra\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "C:\\Users\\apra\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised attention with encoded loss.\n",
      "Writing to C:\\Users\\apra\\Desktop\\FALL 2021\\CY 7990\\model\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\apra\\Desktop\\FALL 2021\\CY 7990\\model\\model_att=encoded_checkpoints\\best_model\n",
      "restoring from trained model...\n",
      "train a new model...\n",
      "[<tf.Variable 'embedding/W:0' shape=(18161, 300) dtype=float32>, <tf.Variable 'pos_embedding/W_pos:0' shape=(26, 26) dtype=float32>, <tf.Variable 'bi-rnn/bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(626, 1200) dtype=float32>, <tf.Variable 'bi-rnn/bidirectional_rnn/fw/lstm_cell/bias:0' shape=(1200,) dtype=float32>, <tf.Variable 'bi-rnn/bidirectional_rnn/bw/lstm_cell/kernel:0' shape=(626, 1200) dtype=float32>, <tf.Variable 'bi-rnn/bidirectional_rnn/bw/lstm_cell/bias:0' shape=(1200,) dtype=float32>, <tf.Variable 'bi-rnn/Variable:0' shape=(600, 20) dtype=float32>, <tf.Variable 'bi-rnn/Variable_1:0' shape=(20,) dtype=float32>, <tf.Variable 'bi-rnn/Variable_2:0' shape=(20,) dtype=float32>, <tf.Variable 'fc-layer-1/W:0' shape=(600, 10) dtype=float32>, <tf.Variable 'fc-layer-1/b:0' shape=(10,) dtype=float32>, <tf.Variable 'fc-layer-2/W:0' shape=(10, 2) dtype=float32>, <tf.Variable 'fc-layer-2/b:0' shape=(2,) dtype=float32>, <tf.Variable 'cross_entropy/ration_W:0' shape=(100, 20) dtype=float32>, <tf.Variable 'cross_entropy/ration_b:0' shape=(20,) dtype=float32>, <tf.Variable 'cross_entropy/alpha_W:0' shape=(100, 20) dtype=float32>, <tf.Variable 'cross_entropy/alpha_b:0' shape=(20,) dtype=float32>]\n",
      "Num Steps:  13860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apra\\Desktop\\FALL 2021\\CY 7990\\toxicity_filter_attacks\\data_utils\\data_helpers.py:115: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  data = np.array(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8050, loss 0.18522103130817413 \n",
      "\n",
      " Evaluation:\n",
      "dev roc_auc: 0.8386485369848461 dev pr_auc: 0.701542963756046\n",
      "best pr auc: 0.701542963756046\n",
      "Saved best model checkpoint.\n",
      "step 8100, loss 0.09785278141498566 \n",
      "\n",
      " Evaluation:\n",
      "dev roc_auc: 0.8801980198019802 dev pr_auc: 0.7207924735675048\n",
      "best pr auc: 0.7207924735675048\n",
      "Saved best model checkpoint.\n",
      "step 8150, loss 0.03524046018719673 \n",
      "\n",
      " Evaluation:\n",
      "dev roc_auc: 0.8452420651901256 dev pr_auc: 0.7118638527303766\n",
      "step 8200, loss -0.010463258251547813 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13344/220940755.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_length_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_length_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13344/220940755.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x_batch, pos_batch, y_batch, sequence_length, pos_sequence_length, attention_batch)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             }\n\u001b[1;32m---> 70\u001b[1;33m             _, step, loss = sess.run(\n\u001b[0m\u001b[0;32m     71\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 feed_dict)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------- model training --------------------------\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    #initialization code required to make tensorflow work on my systemabs\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    with sess.as_default():\n",
    "        model = AbuseClassifier(\n",
    "            max_sequence_length=param.max_sent_len,\n",
    "            num_classes=2,\n",
    "            pos_vocab_size=pos_vocab_size,\n",
    "            init_embed=init_embed,\n",
    "            hidden_size=FLAGS.hidden_size,\n",
    "            attention_size=FLAGS.attention_size,\n",
    "            keep_prob=FLAGS.dropout_keep_prob,\n",
    "            attention_lambda=FLAGS.attention_lambda,\n",
    "            attention_loss_type=FLAGS.attention_loss_type,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "            use_pos_flag=FLAGS.use_pos_flag)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "        grads_and_vars = optimizer.compute_gradients(model.loss, aggregation_method=2)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # save models\n",
    "        if FLAGS.checkpoint == \"\":\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.pardir, \"model\"))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "        else:\n",
    "            out_dir = FLAGS.checkpoint\n",
    "        if (FLAGS.attention_lambda == 0.0):\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"model_noatt_checkpoints\"))\n",
    "        else:\n",
    "            checkpoint_dir = os.path.abspath(\n",
    "                os.path.join(out_dir, \"model_att=\" + FLAGS.attention_loss_type + \"_checkpoints\"))\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver =  tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "        # initalize variables\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        #restore models\n",
    "        try:\n",
    "            saver.restore(sess, os.path.join(checkpoint_dir, \"best_model\"))\n",
    "            print(\"restoring from trained model...\")\n",
    "        except:\n",
    "            \n",
    "            print(\"something went wrong\")\n",
    "        print(\"train a new model...\")\n",
    "        print(tf.compat.v1.trainable_variables())\n",
    "\n",
    "\n",
    "        def train_step(x_batch, pos_batch, y_batch, sequence_length, pos_sequence_length, attention_batch):\n",
    "            feed_dict = {\n",
    "                model.input_word: x_batch,\n",
    "                model.input_pos: pos_batch,\n",
    "                model.input_y: y_batch,\n",
    "                model.sequence_length: sequence_length,\n",
    "                model.input_attention: attention_batch,\n",
    "                model.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, loss = sess.run(\n",
    "                [train_op, global_step, model.loss],\n",
    "                feed_dict)\n",
    "            if (step % FLAGS.evaluate_every == 0):\n",
    "                print(\"step {}, loss {:} \".format(step, loss))\n",
    "\n",
    "\n",
    "        def dev_step(x_dev, pos_dev, y_dev, length_dev, pos_length_dev, writer=None):\n",
    "            dev_scores = []\n",
    "            # loss_list = []\n",
    "            pos = 0\n",
    "            gap = 50\n",
    "            while (pos < len(x_dev)):\n",
    "                x_batch = x_dev[pos:pos + gap]\n",
    "                pos_batch = pos_dev[pos:pos + gap]\n",
    "                y_batch = y_dev[pos:pos + gap]\n",
    "                sequence_length = length_dev[pos:pos + gap]\n",
    "                pos_sequence_length = pos_length_dev[pos:pos + gap]\n",
    "                pos += gap\n",
    "                feed_dict = {\n",
    "                    model.input_word: x_batch,\n",
    "                    model.input_pos: pos_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.sequence_length: sequence_length,\n",
    "                    model.dropout_keep_prob: 0.99999\n",
    "                }\n",
    "                # step, loss, scores = sess.run(\n",
    "                #    [global_step, model.loss, model.prob],\n",
    "                #    feed_dict)\n",
    "                step, scores = sess.run(\n",
    "                    [global_step, model.prob],\n",
    "                    feed_dict)\n",
    "                dev_scores = dev_scores + list([s[0] for s in scores])\n",
    "                # loss_list.append(loss)\n",
    "            gold_scores = [t[0] for t in y_dev]\n",
    "            pred_scores = dev_scores[:]\n",
    "            fpr, tpr, _ = roc_curve(gold_scores, pred_scores, pos_label=1)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            prec, recall, _ = precision_recall_curve(gold_scores, pred_scores, pos_label=1)\n",
    "            pr_auc = auc(recall, prec)\n",
    "            # avg_loss = np.mean(loss_list)\n",
    "            print(\"dev roc_auc:\", roc_auc, \"dev pr_auc:\", pr_auc)\n",
    "            return roc_auc, pr_auc  # , avg_loss\n",
    "\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train, pos_train, length_train, pos_length_train, attention_train)), FLAGS.batch_size,\n",
    "            FLAGS.num_epochs)\n",
    "        best_auc = 0.10\n",
    "\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, pos_batch, length_batch, pos_length_batch, attention_batch = zip(*batch)\n",
    "            train_step(x_batch, pos_batch, y_batch, length_batch, pos_length_batch, attention_batch)\n",
    "            current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
    "            if (current_step % FLAGS.evaluate_every == 0):\n",
    "                print(\"\\n Evaluation:\")\n",
    "                roc_auc, pr_auc = dev_step(x_dev, pos_dev, y_dev, length_dev, pos_length_dev)\n",
    "                # model selection criteria: roc_auc\n",
    "                # if (best_auc < roc_auc):\n",
    "                #    best_auc = roc_auc\n",
    "                if (best_auc < pr_auc):\n",
    "                    best_auc = pr_auc\n",
    "                    print(\"best pr auc:\", best_auc)\n",
    "                    checkpoint_prefix = os.path.join(checkpoint_dir, \"best_model\")\n",
    "                    path = saver.save(sess, checkpoint_prefix)\n",
    "                    print(\"Saved best model checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9b29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
