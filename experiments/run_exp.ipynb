{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d3d625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:50:52.069245Z",
     "start_time": "2021-12-06T05:50:51.870243Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-08a92609fc96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_helpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloadVocabEmb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_data_helpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenPOSFeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mexperiments\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from data_utils.data_helpers import genFeatures, loadVocabEmb\n",
    "from data_utils.tag_data_helpers import genPOSFeatures\n",
    "from experiments import params\n",
    "from model.abuse_classifier import AbuseClassifier\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"pos_vocab_size\", 26, \"Vocab size of POS tags\")\n",
    "tf.flags.DEFINE_integer(\"pos_embedding_dim\", 25, \"Dimensionality of pos tag embedding (default: 20)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 1.0, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"attention_lambda\", 0, \"Supervised attention lambda (default: 0.05)\")\n",
    "tf.flags.DEFINE_string(\"attention_loss_type\", 'encoded', \"loss function of attention\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.02, \"L2 regularization lambda (default: 0.05)\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\", 300, \"Dimensionality of RNN cell (default: 300)\")\n",
    "tf.flags.DEFINE_integer(\"pos_hidden_size\", 25, \"Dimensionality of POS-RNN cell\")\n",
    "tf.flags.DEFINE_integer(\"attention_size\", 20, \"Dimensionality of attention scheme (default: 50)\")\n",
    "tf.flags.DEFINE_boolean(\"use_pos_flag\", True, \"use the sequence of POS tags\")\n",
    "# Training parameters -- evaluate_every should be 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 32)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 60, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 500000, \"Save model after this many steps (default: 100)\")\n",
    "# tf.flags.DEFINE_float(\"train_ratio\", 1.0, \"Ratio of training data\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint\", '', \"model\")\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "def load_vocab():\n",
    "    vocabulary, pos_vocabulary, init_embed = loadVocabEmb()\n",
    "    return vocabulary, pos_vocabulary, init_embed\n",
    "\n",
    "\n",
    "def load_data(dump_folder_path, data_type, verbose=True):\n",
    "    assert data_type in [\"train\", \"test\"]\n",
    "    with open(os.path.join(dump_folder_path, \"vocab.pkl\"), \"rb\") as handle:\n",
    "        vocabulary = pickle.load(handle)\n",
    "    with open(os.path.join(dump_folder_path, \"pos_vocab.pkl\"), \"rb\") as handle:\n",
    "        pos_vocabulary = pickle.load(handle)\n",
    "    with open(os.path.join(dump_folder_path, data_type + \"_comm.data\"), \"rb\") as handle:\n",
    "        sentences, labels = pickle.load(handle)\n",
    "    with open(os.path.join(dump_folder_path, data_type + \"_comm_pos.data\"), \"rb\") as handle:\n",
    "        pos_sentences = pickle.load(handle)\n",
    "    with open(os.path.join(dump_folder_path, data_type + \"_attention.data\"), \"rb\") as handle:\n",
    "        attention = pickle.load(handle)\n",
    "    # generate features & labels\n",
    "    x, length, attention = genFeatures(sentences, attention, params.max_sent_len, vocabulary)\n",
    "    pos, pos_length = genPOSFeatures(pos_sentences, params.max_sent_len, pos_vocabulary)\n",
    "    y = np.array(labels)\n",
    "    if verbose:\n",
    "        print(\"load {} data, input sent size: {}, input POS size: {}, label size: {}\".format(\n",
    "            data_type, np.array(x).shape, np.array(pos).shape, np.array(y).shape))\n",
    "    x_test, length_test, attention_test, pos_test, pos_length_test, y_test = x, length, attention, pos, pos_length, y\n",
    "    return x_test, length_test, attention_test, pos_test, pos_length_test, y_test\n",
    "\n",
    "\n",
    "def get_predictions(model_path, data_path):\n",
    "    vocabulary, pos_vocabulary, init_embed = load_vocab()\n",
    "    x_test, length_test, attention_test, pos_test, pos_length_test, y_test = load_data(data_path, \"test\", verbose=False)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            model = AbuseClassifier(\n",
    "                max_sequence_length=params.max_sent_len,\n",
    "                num_classes=2,\n",
    "                pos_vocab_size=FLAGS.pos_vocab_size,\n",
    "                init_embed=init_embed,\n",
    "                hidden_size=FLAGS.hidden_size,\n",
    "                attention_size=FLAGS.attention_size,\n",
    "                keep_prob=FLAGS.dropout_keep_prob,\n",
    "                attention_lambda=FLAGS.attention_lambda,\n",
    "                attention_loss_type=FLAGS.attention_loss_type,\n",
    "                l2_reg_lambda=0.1,\n",
    "                use_pos_flag=FLAGS.use_pos_flag)\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            saver.restore(sess, model_path)\n",
    "\n",
    "            dev_scores = []\n",
    "            pos = 0\n",
    "            gap = 50\n",
    "            while pos < len(x_test):\n",
    "                x_batch = x_test[pos:pos + gap]\n",
    "                pos_batch = pos_test[pos:pos + gap]\n",
    "                y_batch = y_test[pos:pos + gap]\n",
    "                length_batch = length_test[pos:pos + gap]\n",
    "                pos_length_batch = pos_length_test[pos:pos + gap]\n",
    "                pos += gap\n",
    "                # score sentences\n",
    "                feed_dict = {\n",
    "                    model.input_word: x_batch,\n",
    "                    model.input_pos: pos_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.sequence_length: length_batch,\n",
    "                    model.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, scores = sess.run([global_step, model.prob], feed_dict)\n",
    "                dev_scores = dev_scores + list([s[0] for s in scores])\n",
    "    return dev_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ca506",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    model_save_folder_name = (sys.argv[1] if len(sys.argv) > 1 else False) or \"model_noatt_checkpoints\"\n",
    "\n",
    "    print(f\"Running model from {model_save_folder_name}\")\n",
    "\n",
    "    model_folder_path = '../model'\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(model_folder_path, model_save_folder_name))\n",
    "    model_path = os.path.join(checkpoint_dir, \"best_model\")\n",
    "\n",
    "    data_path = '../dump'\n",
    "    print(f\"Using data from {data_path}\")\n",
    "\n",
    "    predictions = get_predictions(model_path, data_path)\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Ran model from {model_save_folder_name}, using data from {data_path}\")\n",
    "\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)\n",
    "\n",
    "    print(\"Classification:\")\n",
    "    y_pred = list(map(lambda p: 1 if p > 0.5 else 0, predictions))\n",
    "    print(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
